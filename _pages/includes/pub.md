# üìù Selected Publications

<!-- My full paper list can be found at <a href='https://scholar.google.com/citations?user=lFZpfE4AAAAJ'><img src="https://img.shields.io/endpoint?url=https://cdn.jsdelivr.net/gh/circleLZY/circleLZY.github.io@google-scholar-stats/gs_data_shieldsio.json&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->

## Remote Sensing Change Detection

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/mtkd.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework](https://arxiv.org/pdf/2502.13407) \\
**Ziyuan Liu**, Ruifei Zhu, Long Gao, Yuanxiu Zhou, Jingyu Ma, Yuantao Gu

[**Code**](https://github.com/circleLZY/MTKD-CD) ![](https://img.shields.io/github/stars/circleLZY/MTKD-CD?style=social)
  -  We introduce **JL1-CD**, a new sub-meter, all-inclusive open-source CD dataset comprising 5,000 pairs of remote sensing image patches with a resolution of 0.5‚Äì0.75 meters. 
  -  We propose a multi-teacher knowledge distillation (MTKD) framework, which significantly improves the performance of CD models with various network architectures and parameter sizes without increasing any computational or time cost during inference.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TGRS 2024</div><img src='images/dsrkd.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DSRKD: Joint Despecking and Super-Resolution of SAR Images via Knowledge Distillation](https://ieeexplore.ieee.org/abstract/document/10606284) \\
**Ziyuan Liu**, Shaoping Wang, Ying Li, Yuantao Gu, Quan Yu

  - We propose the DSRKD network, with an encoder‚Äìupscaling‚Äìdecoder architecture that allows for SR at various resolution scales. The encoder and decoder can also be replaced with various more complex modules, and various distillation methods can be easily added, making it a simple and efficient baseline for despeckling SR.
  - We innovatively combine the SR of speckled images with denoising using knowledge distillation (KD), significantly improving the performance of the model without introducing any additional computational overhead during inference.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">GRSL 2024</div><img src='images/grsl.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SAR Image Compression With Inherent Denoising Capability Through Knowledge Distillation](https://ieeexplore.ieee.org/abstract/document/10495338) \\
**Ziyuan Liu**, Shaoping Wang , Yuantao Gu

  - We innovatively utilize a KD mechanism to combine compression with denoising for speckled images. Our method exhibits superior performance compared to other methods on both synthetic and SAR datasets.
  - The distillation mechanism is applicable to various learned image compression algorithms and consistently enhances their performance.
  - This enhancement is achieved without introducing extra complexity in terms of computational time and memory utilization during inference.
</div>
</div>